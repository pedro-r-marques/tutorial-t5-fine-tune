{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Z8Yp_pxoofWQ"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Google cloud variables.\n",
        "\"\"\"\n",
        "GCLOUD_PROJECT=<GOOGLE_CLOUD_PROJECT>\n",
        "GCS_BUCKET=<CLOUD_STORAGE_BUCKET>\n",
        "GCS_MODEL_DIR='so-quality/t5-model/01'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxccQQ2MOaH6"
      },
      "source": [
        "# environment setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wA2el8q8oqS2",
        "outputId": "29d9f609-b13c-4774-84da-039101fcb175"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated property [core/project].\n"
          ]
        }
      ],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "!gcloud config set project $GCLOUD_PROJECT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "985dNuwUOgkm",
        "outputId": "128ef857-8118-43eb-8d7f-4f5654396f6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers==4.11.3 in /usr/local/lib/python3.7/dist-packages (4.11.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.11.3) (1.19.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.11.3) (0.0.19)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.11.3) (0.0.46)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.11.3) (21.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.11.3) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.11.3) (3.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.11.3) (4.8.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.11.3) (4.62.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.11.3) (6.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.11.3) (0.10.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.11.3) (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers==4.11.3) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.11.3) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.11.3) (3.6.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.11.3) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.11.3) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.11.3) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.11.3) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.11.3) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.11.3) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.11.3) (7.1.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n"
          ]
        }
      ],
      "source": [
        "%pip install transformers==4.11.3\n",
        "%pip install sentencepiece\n",
        "%pip install -q -U tf-models-official"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVkAZLa0OkoC",
        "outputId": "238f4554-4785-4884-ddd4-6d27dccdb644"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.60.245.2:8470\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.60.245.2:8470\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using TPU\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from official.nlp import optimization\n",
        "\n",
        "if os.environ.get('COLAB_TPU_ADDR'):\n",
        "  cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\n",
        "  tf.config.experimental_connect_to_cluster(cluster_resolver)\n",
        "  tf.tpu.experimental.initialize_tpu_system(cluster_resolver)\n",
        "  strategy = tf.distribute.TPUStrategy(cluster_resolver)\n",
        "  print('Using TPU')\n",
        "elif tf.config.list_physical_devices('GPU'):\n",
        "  strategy = tf.distribute.MirroredStrategy()\n",
        "  print('Using GPU')\n",
        "else:\n",
        "  strategy = tf.distribute.get_strategy()\n",
        "  print('Running on CPU is not recommended.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JSpUOEkOoUC"
      },
      "source": [
        "## dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "QM04SAvuOsDw"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def tf_record_decoder(encoded):\n",
        "    features = {\n",
        "        \"input_ids\": tf.io.FixedLenFeature([512], tf.int64),\n",
        "        \"attention_mask\": tf.io.FixedLenFeature([512], tf.int64),\n",
        "        \"labels\": tf.io.FixedLenFeature([2], tf.int64),\n",
        "        \"decoder_attention_mask\": tf.io.FixedLenFeature([2], tf.int64),\n",
        "        \"class\": tf.io.FixedLenFeature([1], tf.int64),\n",
        "    }\n",
        "    tf_record = tf.io.parse_single_example(encoded, features)\n",
        "    return tf_record\n",
        "\n",
        "def dataset_prepare(dataset, batch_size=32, training=False):\n",
        "    dataset = dataset.map(tf_record_decoder)\n",
        "    dataset = dataset.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "    if training:\n",
        "        dataset = dataset.shuffle(1*1024)\n",
        "\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    return dataset\n",
        "\n",
        "def eval_mapper(batch):\n",
        "    \"\"\" Map training entries in the format expected by model.predict\n",
        "        i.e. the decoder_inputs are set with the <bos> token (id = 1)\n",
        "        Since the expected classes are expressed as a single token\n",
        "        we can retrieve the output with a single call to predict rather\n",
        "        than using the more expensive text generation strategy that T5\n",
        "        uses to predict sentences.\n",
        "    \"\"\"\n",
        "    batch_size = tf.shape(batch['input_ids'])[0]\n",
        "    inputs = {\n",
        "        'input_ids': batch['input_ids'],\n",
        "        'attention_mask': batch['attention_mask'],\n",
        "        'decoder_input_ids': tf.zeros((batch_size, 1), dtype=tf.int32),\n",
        "        'decoder_attention_mask': tf.ones((batch_size, 1)),\n",
        "    }\n",
        "    return inputs\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "pAmL_V5sOxcb"
      },
      "outputs": [],
      "source": [
        "ds_tr_valid = tf.data.TFRecordDataset(f'gs://{GCS_BUCKET}/so-quality/dataset_t5_valid.tfrecord')\n",
        "with strategy.scope():\n",
        "    ds_valid = dataset_prepare(ds_tr_valid)\n",
        "    ds_eval = ds_valid.map(eval_mapper)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4XQZyN6QMJs"
      },
      "source": [
        "## Load the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIZTvkchQaSt",
        "outputId": "4d00bfd3-f5d9-4482-9203-155152d1bd8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Copying gs://ardent-strength-306418-data/so-quality/t5-model/01/config.json...\n",
            "/ [0 files][    0.0 B/  1.3 KiB]                                                \r/ [1 files][  1.3 KiB/  1.3 KiB]                                                \r\n",
            "Operation completed over 1 objects/1.3 KiB.                                      \n",
            "Copying gs://ardent-strength-306418-data/so-quality/t5-model/01/tf_model.h5...\n",
            "| [1 files][850.8 MiB/850.8 MiB]   51.4 MiB/s                                   \n",
            "Operation completed over 1 objects/850.8 MiB.                                    \n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.makedirs('t5-tuned', exist_ok=True)\n",
        "\n",
        "!gsutil cp gs://$GCS_BUCKET/$GCS_MODEL_DIR/config.json t5-tuned/\n",
        "!gsutil cp gs://$GCS_BUCKET/$GCS_MODEL_DIR/tf_model.h5 t5-tuned/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "FNpPiEh_TB5S"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class PredictionModel(tf.keras.Model):\n",
        "    \"\"\" The model call function is executed in the TPU.\n",
        "\n",
        "        This wrapper exists so that the argmax computation on logits is performed\n",
        "        on the TPU and only the token indices are transfered between TPU\n",
        "        and colab CPU. colab will run out of memory otherwise. Or one is\n",
        "        forced to execute the predict calls once batch at a time which leads\n",
        "        to graph setup/tear down costs.\n",
        "    \"\"\"\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self._model = model\n",
        "    \n",
        "    def call(self, inputs):\n",
        "        outputs = self._model(inputs)\n",
        "        return tf.argmax(outputs['logits'], axis=-1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Tq85-Ys5RqIy"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class ClassDecoder(object):\n",
        "    \"\"\" Translate the class tokens into class ids.\n",
        "    \"\"\"\n",
        "    TOKENS = ['none', 'low', 'high']\n",
        "    def __init__(self, tokenizer):\n",
        "        self.tok_ids = [tokenizer.encode(tok)[0] for tok in self.TOKENS]\n",
        "\n",
        "    def _index(self, x):\n",
        "        try:\n",
        "            return self.tok_ids.index(x)\n",
        "        except ValueError:\n",
        "            return -1\n",
        "    \n",
        "    def __call__(self, y_pred_ids):\n",
        "        result = [self._index(x) for x in y_pred_ids]\n",
        "        return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "-yBjubzZQIU2"
      },
      "outputs": [],
      "source": [
        "from transformers import T5Tokenizer, TFT5ForConditionalGeneration\n",
        "\n",
        "with strategy.scope():\n",
        "    tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
        "    model = TFT5ForConditionalGeneration.from_pretrained('./t5-tuned/')\n",
        "    xmodel = PredictionModel(model)\n",
        "\n",
        "    decoder = ClassDecoder(tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlPG8XQtVcCn"
      },
      "source": [
        "## Execute model predictions\n",
        "\n",
        "Given that we are using a class with a single token the model can be run once per example in order to predict the next token output when the decoder contains only the <bos> token.\n",
        "\n",
        "We compute the confusion matrix and accuracy score on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gojEw2tTpoQ",
        "outputId": "42f3e705-fa46-4178-a4eb-af328e1e7ea9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, <tf.Tensor 'cond_8/Identity:0' shape=(None, 512) dtype=int64>, <tf.Tensor 'cond_8/Identity_1:0' shape=(None, 1) dtype=float32>, <tf.Tensor 'cond_8/Identity_2:0' shape=(None, 1) dtype=int32>, <tf.Tensor 'cond_8/Identity_3:0' shape=(None, 512) dtype=int64>]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "469/469 [==============================] - 78s 99ms/step\n"
          ]
        }
      ],
      "source": [
        "with strategy.scope():\n",
        "    y_pred_ids = xmodel.predict(ds_eval, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "5n_YnU63TscF"
      },
      "outputs": [],
      "source": [
        "decoder = ClassDecoder(tokenizer)\n",
        "y_pred = np.array(decoder(y_pred_ids))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ab5K9pT9UKh4",
        "outputId": "1da19e4f-1fbc-44dd-f46c-84d4939a67cc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "469it [00:01, 356.95it/s]\n"
          ]
        }
      ],
      "source": [
        "import tqdm\n",
        "\n",
        "y_true_list = []\n",
        "for batch in tqdm.tqdm(ds_valid):\n",
        "    y_true_list.append(batch['class'].numpy().reshape(-1))\n",
        "y_true = np.concatenate(y_true_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLL6dAYATRaD",
        "outputId": "11a6a2eb-cfcf-4b5a-e6a6-2efdb515283d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[4336,    0,  664],\n",
              "       [   6, 4994,    0],\n",
              "       [ 284,    0, 4716]])"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import sklearn.metrics\n",
        "cm = sklearn.metrics.confusion_matrix(y_true, y_pred)\n",
        "cm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZaFK2SjUM_-",
        "outputId": "5dc579db-ab88-4a54-e109-f30a6bd5b62c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.9364"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sklearn.metrics.accuracy_score(y_true, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldvOf6qpV_9q",
        "outputId": "d8e4fcce-d0e3-447a-e4ec-57eb85bee144"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.9361627906976744"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Compute the accuracy metric with the validation examples that we not used to determine the best checkpoint.\n",
        "\n",
        "sklearn.metrics.accuracy_score(y_true[200 * 32:], y_pred[200 * 32:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIDi0xy5WbGS"
      },
      "source": [
        "# Debug samples\n",
        "\n",
        "Examine bellow a random sample of the errors the model makes on the hold out set.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "sIcDjm7LWZNG"
      },
      "outputs": [],
      "source": [
        "misclassified = np.where(y_true[200 * 32:] != y_pred[200 * 32:])[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "Oy47HayOW2KV"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, HTML\n",
        "\n",
        "def _get_index(dataset, y_true, y_pred, index, batch_size=32):\n",
        "    n_bucket = index // batch_size\n",
        "    bi = index % batch_size\n",
        "    if n_bucket > 0:\n",
        "        dataset = dataset.skip(n_bucket)\n",
        "    for batch in dataset.take(1):\n",
        "        token_ids = batch['input_ids'][bi]\n",
        "        mask = batch['attention_mask'][bi]\n",
        "        zi = np.where(mask == 0)[0]\n",
        "        if zi.size:\n",
        "            token_ids = token_ids[:zi[0]]\n",
        "        s = tokenizer.decode(token_ids)\n",
        "        tx = batch['class'].numpy().reshape(-1)[bi]\n",
        "        px = y_pred[index]\n",
        "    \n",
        "    return tx, px, s\n",
        "\n",
        "def show_debug_samples(dataset, y_true, y_pred, indices):\n",
        "    html = \"\"\"\n",
        "    <table>\n",
        "        <thead>\n",
        "            <tr>\n",
        "                <th>index</th>\n",
        "                <th>y_true</th>\n",
        "                <th>y_pred</th>\n",
        "                <th>tokens</th>\n",
        "            </tr>\n",
        "        </thead>\n",
        "        <tbody>\n",
        "    \"\"\"\n",
        "\n",
        "    for ix in indices:\n",
        "        values = _get_index(dataset, y_true, y_pred, ix)\n",
        "        html += '<tr><td>{0}</td>'.format(ix)\n",
        "        for v in values:\n",
        "            html += '<td>{0}</td>'.format(v)\n",
        "        html += '</tr>'\n",
        "\n",
        "    html += \"\"\"\n",
        "        </tbody>\n",
        "    </table>\n",
        "    \"\"\"\n",
        "    display(HTML(html))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ErZiSybfXI_i",
        "outputId": "55b0b69f-3e63-4a85-c735-39373f21690e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <table>\n",
              "        <thead>\n",
              "            <tr>\n",
              "                <th>index</th>\n",
              "                <th>y_true</th>\n",
              "                <th>y_pred</th>\n",
              "                <th>tokens</th>\n",
              "            </tr>\n",
              "        </thead>\n",
              "        <tbody>\n",
              "    <tr><td>12076</td><td>0</td><td>2</td><td>quality: Reverse Integer Leetcode - Explain overflow issue</s> <unk> p>I am new to coding and practicing leetcode problems. Integer reverse problem talks about overflow.<unk> /p> <unk> p>I have searched and most of the discussion about how to handle the overflow. Could someone explain what is this overflow and why is this caused?<unk> /p></s></td></tr><tr><td>14326</td><td>0</td><td>2</td><td>quality: Conditional return with no else</s> <unk> p>I'm try to do something like that in Python:<unk> /p> <unk> pre><unk> code>return None if a is None <unk> /code><unk> /pre> <unk> p>Instead of having:<unk> /p> <unk> pre><unk> code>if a is None: return None <unk> /code><unk> /pre> <unk> p>But I get an invalid syntax error.<unk> /p> <unk> p>I tried to use <unk> code>pass<unk> /code>:<unk> /p> <unk> pre><unk> code>return None if a is None else pass <unk> /code><unk> /pre> <unk> p>But it doesn't work as well. Is there a pythonian way to do it?<unk> /p></s></td></tr><tr><td>13361</td><td>0</td><td>2</td><td>quality: Add font-awesome icon to option in select</s> <unk> p>Is it possible to add font-awesome icon to<unk> code>&lt;option&gt;<unk> /code> in <unk> code>&lt;select&gt;<unk> /code>?<unk> /p> <unk> pre><unk> code> &lt;body ng-app&gt; &lt;i class=\"fa fa-camera-retro\"&gt;&lt;/i&gt; fa-camera-retro&lt;br&gt; &lt;select ng-model=\"choice\" class=\"fa\"&gt; &lt;option value=\"\"&gt;Choose&lt;/option&gt; &lt;option value=\"icon camera\"&gt;icon camera&lt;/option&gt; &lt;option value=\"icon bell\"&gt;icon bell&lt;/option&gt; &lt;option value=\"icon bicycle\"&gt;icon bicycle&lt;/option&gt; &lt;/select&gt; <unk> /code><unk> /pre></s></td></tr><tr><td>12390</td><td>0</td><td>2</td><td>quality: How does one force a C++ move operator without std:whatever</s> <unk> p>I have a very large code base that has been around a while. I've been updating it with selective use of new language features. I was going to play around with move constructors but I can't even come up with a scenario convoluted to make one happen. I'm not going to throw code into my code base that I can't even unit test.<unk> /p> <unk> p>And, I cannot use std:move, because I don't use the standard libraries at all. I have my own standard libraries, along with my own everything else (about a million lines of code.) Everything is hidden within a virtual kernel that doesn't expose any language or platform headers to the outside world.<unk> /p> <unk> p>So I can't use std::move. Hopefully it doesn't do anything magical that I can't do? And I guess something similar would also be required to handle move constructors in the face of base classes (almost always the case.) If it does do something magical, that's the library leaking into the language and wouldn't be good.<unk> /p> <unk> p>Apparently there's no compile settings on Visual C++ (2017 in my case) to disable it from eliding constructors, which is presumably why I can't manage to do anything to even cause it.<unk> /p> <unk> p>Of course that also raises the question of whether it's useless if it requires this much effort (even in non-optimized mode) to make it even happen.<unk> /p></s></td></tr><tr><td>8385</td><td>2</td><td>0</td><td>quality: Java spread operator</s> <unk> p>I am not sure of the vocabulary I am using here, please correct me if I'm wrong. <unk> /p> <unk> p>In Javascript, I had the following code:<unk> /p> <unk> pre><unk> code>let args = [1,2,3]; function doSomething (a, b, c) <unk> return a + b + c; <unk> doSomething(...args); <unk> /code><unk> /pre> <unk> p>As you can see, when calling <unk> code>doSomething<unk> /code>, I am able to use the <unk> code>...<unk> /code> spread operator in order to \"transform\" my arguments into <unk> code>1, 2, 3<unk> /code>.<unk> /p> <unk> p>Now, I'm trying to do the same thing with Java. <unk> /p> <unk> p>Let's say I have a <unk> code>Foo<unk> /code> class:<unk> /p> <unk> pre><unk> code>public class Foo <unk> public int doSomething (int a, int b, int c) <unk> return a + b + c; <unk> <unk> <unk> /code><unk> /pre> <unk> p>And now I want to call the <unk> code>doSomething<unk> /code>:<unk> /p> <unk> pre><unk> code>int[] args = <unk> 1, 2, 3<unk> ; <unk> /code><unk> /pre> <unk> p>I'd like to use something like <unk> code>doSomething (...args)<unk> /code> instead of calling <unk> code>doSomething(args[0], args[1], args[2])<unk> /code>.<unk> /p> <unk> p>I saw that this is possible in the declaration of functions, but I'd like not to change the implementation of such a function.<unk> /p></s></td></tr><tr><td>10219</td><td>0</td><td>2</td><td>quality: why recv accept mort than expected number of bytes?</s> <unk> p>When i call recv to receive exactly 7 bytes i recv more than bytes expected?<unk> /p> <unk> p>I don't know how bufsize parameter work.<unk> /p> <unk> pre><unk> code>for i in range(data2[1]): try: tuple = envois.recv(7) time.sleep(0.1) except Exception as e: print(e) if (tuple[-2:]!= myport): tuples.append(tuple) print(\"tuples : <unk> x<unk> \".format(x = tuples)) last_msg = envois.recv(3) envois.close() return (last_msg,tuples) <unk> /code><unk> /pre></s></td></tr><tr><td>13641</td><td>0</td><td>2</td><td>quality: Is there a way to get all keys that match a string in a json file's values and output them to a text file?</s> <unk> p>I'm trying to get all keys' values that equal \"url\" ignoring nesting from a JSON file and then output them to a text file. How would I go about doing this?<unk> /p> <unk> p>I'm running Python 3.7 and cannot seem to find a solution.<unk> /p> <unk> pre><unk> code>r = requests.get('https://launchermeta.mojang.com/mc/game/version_manifest.json') j = r.json() <unk> /code><unk> /pre> <unk> p>The result expected from this would be a text file filled with links from this json file.<unk> /p> <unk> pre><unk> code>https://launchermeta.mojang.com/v1/packages/31fa028661857f2e3d3732d07a6d36ec21d6dbdc/a1.2.3_02.json https://launchermeta.mojang.com/v1/packages/2dbccc4579a4481dc8d72a962d396de044648522/a1.2.3_01.json https://launchermeta.mojang.com/v1/packages/48f077bf27e0a01a0bb2051e0ac17a96693cb730/a1.2.3.json etc. <unk> /code><unk> /pre></s></td></tr><tr><td>13417</td><td>0</td><td>2</td><td>quality: How to create a UIButton similar to type.system?</s> <unk> p>How to create a <unk> code>UIButton<unk> /code> with tap effect similar to <unk> code>UIButton(type:.system)<unk> /code>, but without specifying <unk> code>.system<unk> /code> type?<unk> /p> <unk> pre><unk> code>let btn = UIButton(frame: CGRect.zero) // How to set the tap effect? <unk> /code><unk> /pre> <unk> p>How to set the tap effect which blinks the text when tapped?<unk> /p></s></td></tr><tr><td>7999</td><td>0</td><td>2</td><td>quality: show uidatepicker, when I tap on view</s> <unk> p>I have a view with UITapGestureRecognizer<unk> /p> <unk> p>I want to show uidatepicker, when I tap on view<unk> /p> <unk> p>How can I do it?<unk> /p> <unk> p>thanks in advance<unk> /p></s></td></tr><tr><td>11714</td><td>2</td><td>0</td><td>quality: Back and forth loop Python</s> <unk> p>I want to create an infinite loop that counts up and down from 0 to 100 to 0 (and so on) and only stops when some convergence criterion inside the loop is met, so basically something like this:<unk> /p> <unk> pre><unk> code>for i in range(0, infinity): for j in range(0, 100, 1): print(j) # (in my case 100 lines of code) for j in range(100, 0, -1): print(j) # (same 100 lines of code as above) <unk> /code><unk> /pre> <unk> p>Is there any way to merge the two for loops over j into one so that I don't have write out the same code inside the loops twice?<unk> /p></s></td></tr><tr><td>7440</td><td>0</td><td>2</td><td>quality: Pandas: Delete row based on a condition of more than one column</s> <unk> p>I have a DataFrame \"df\" with three columns named: \"Particle\", \"Frequency1\", \"Frequency2\" and a lot of rows.<unk> /p> <unk> p>I want to delete the rows where Frequency1 and Frequency2 are simoustaneously equal to 0.<unk> /p> <unk> p>What is the sintax for doing this?<unk> /p></s></td></tr><tr><td>9485</td><td>0</td><td>2</td><td>quality: Why doesn't itertools groupby work as expected?</s> <unk> p>I am trying to separate a list of integers into odd and even groups.<unk> /p> <unk> pre><unk> code>&gt;&gt;&gt; from itertools import groupby &gt;&gt;&gt; L = [1,2,3,4] &gt;&gt;&gt; grouped = [list(g) for k,g in groupby(L, key=lambda x: x % 2)] &gt;&gt;&gt; grouped [[1], [2], [3], [4]] <unk> /code><unk> /pre> <unk> p>Clearly, not what I want. However, if I sort L first, using the same lambda key, it works as intended:<unk> /p> <unk> pre><unk> code>&gt;&gt;&gt; L.sort(key=lambda x: x % 2) &gt;&gt;&gt; L [2, 4, 1, 3] &gt;&gt;&gt; grouped = [list(g) for k,g in groupby(L, key=lambda x: x % 2)] &gt;&gt;&gt; grouped [[2, 4], [1, 3]] <unk> /code><unk> /pre> <unk> p>I don't understand why presorting is necessary. It seems groupby should iterate through the integers in list, assign each a value based on key function, and then group them -- regardless of the list order.<unk> /p></s></td></tr><tr><td>6481</td><td>2</td><td>0</td><td>quality: Wrapping C++ code with python (manually)</s> <unk> p>I have a main file(main.cpp) and a header file(nodes.hpp). The main file takes N(any positive integer) as input argument and by using the functions of header file it gives output say 'x &amp; y' (both double).<unk> /p> <unk> p><unk> strong>Note:<unk> /strong> <unk> /p> <unk> ol> <unk> li>Both main and header files are written in C++.<unk> /li> <unk> li>Both main and header files instead of using data structues as arrays,vectors, make use of Eigen Library.<unk> /li> <unk> /ol> <unk> p>I have to write a python wrapper for them, I have working knowledge of python but have never used any wrapper. <unk> /p> <unk> p>Can anybody please refer or give some notes about using python wrpper for such code? <unk> /p></s></td></tr><tr><td>14829</td><td>0</td><td>2</td><td>quality: Create user input validation loop in python</s> <unk> p>I want the user to enter an integer as a server count when creating EC2 instances in AWS.<unk> /p> <unk> p>I wrote the following loop:<unk> /p> <unk> pre><unk> code>max_count = '' while not max_count: max_count = input(\"Enter how many EC2 Servers: \") try: max_count = int(max_count) except: max_count = input(\"Enter how many EC2 Servers: \") <unk> /code><unk> /pre> <unk> p>If I enter a non integer (like a word) once, it works and prompts you again. But if you enter the wrong input twice you get an error:<unk> /p> <unk> pre><unk> code>Traceback (most recent call last): File \".<unk> aws_create_ec2_simple.py\", line 100, in &lt;module&gt; main() File \".<unk> aws_create_ec2_simple.py\", line 91, in main aws_account, region, max_count, image_id, instance_type, vpc_id, sg_list, subnet_ids, public_ip, private_ip, tenancy, monitoring_enabled, user_data = user_input() File \"C:<unk> Users<unk> User<unk> OneDrive - Company Technologies<unk> Desktop<unk> important_folders<unk> Company<unk> git<unk> cloud_scripts<unk> aws_scripts<unk> python<unk> aws_tools<unk> user_input.py\", line 30, in user_input for count in range(max_count): TypeError:'str' object cannot be interpreted as an integer <unk> /code><unk> /pre> <unk> p>How can I get this to loop the input until the user inputs an integer?<unk> /p></s></td></tr><tr><td>11958</td><td>0</td><td>2</td><td>quality: KVO observable properties position in classes in Swift</s> <unk> p>I'm going through a Ray Wenderlich book, and at one point it mentions marking observable properties as requiring dynamic dispatch such that:<unk> /p> <unk> pre><unk> code> @objc dynamic private(set) var sources: [Source] = [] @objc dynamic private(set) var articles: [Article] = [] <unk> /code><unk> /pre> <unk> p>Now I would naturally put them at the top of the relevant class. However, the example has them further down, as follows. What is the rationale for this?<unk> /p> <unk> pre><unk> code>import Foundation class NewsAPI: NSObject <unk> static let service = NewsAPI() private struct Response: Codable <unk> let sources: [Source]? let articles: [Article]? <unk> private enum API <unk> private static let basePath = \"https://newsapi.org/v1\" /* Head on over to https://newsapi.org/register to get your<extra_id_1> ).sources <unk> self.sources = sources <unk> <unk> <unk> func fetchArticles(for source: Source) <unk> let formatter = ISO8601DateFormatter() let customDateHandler: (Decoder) throws -&gt; Date = <unk> decoder in var string = try decoder.singleValueContainer().decode(String.self) string.deleteMillisecondsIfPresent() guard let date = formatter.date(from: string) else <unk> return Date() <unk> return date <unk> API.articles(source).fetch <unk> data in let decoder = JSONDecoder() decoder.dateDecodingStrategy =.custom(customDateHandler) if let articles = try! decoder.decode(Response.self, from: data).articles <unk> self.articles = articles <unk> <unk> <unk> func resetArticles() <unk> articles = [] <unk> <unk> <unk> /code><unk> /pre></s></td></tr><tr><td>10100</td><td>0</td><td>2</td><td>quality: Running total for each day</s> <unk> p>I need to create a running total for each day in excel or R Studio for this table and I cannot figure out how to do it. So for 2/28 it would start at 1 and then 2. Then for 2/27 it would start at 1 again and proceed down and so on restarting the total each day.<unk> /p> <unk> p><unk> div class=\"snippet\" data-lang=\"js\" data-hide=\"false\" data-console=\"true\" data-babel=\"false\"> <unk> div class=\"snippet-code\"> <unk> pre class=\"snippet-code-html lang-html prettyprint-override\"><unk> code>Date Total 2/28/2018 2/28/2018 2/27/2018 2/27/2018 2/27/2018 2/27/2018 2/27/2018 2/27/2018 2/27/2018 2/27/2018 2/25/2018 2/25/2018 2/25/2018 2/25/2018 2/25/2018 2/25/2018 2/24/2018 2/24/2018 2/24/2018 2/24/2018 2/24/2018 2/24/2018 2/24/2018 2/24/2018 2/24/2018 2/24/2018 2/23/2018 2/23/2018 2/23/2018 2/23/2018 2/23/2018 2/23/2018 2/23/2018 2/23/2018 2/23/2018 <unk> /code><unk> /pre> <unk> /div> <unk> /div> <unk> /p></s></td></tr>\n",
              "        </tbody>\n",
              "    </table>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import random\n",
        "random.seed(42)\n",
        "\n",
        "indices = misclassified + (200 * 32)\n",
        "index_list = indices.tolist()\n",
        "random.shuffle(index_list)\n",
        "show_debug_samples(ds_valid, y_true, y_pred, index_list[:16])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "name": "so-quality-eval.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
